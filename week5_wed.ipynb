{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Welcome to Week 5 ##\n",
    "# Today we are going to try some validation\n",
    "\n",
    "##Imports##\n",
    "__author__ = 'bdyetton'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load and prepare data\n",
    "all_charts = pd.read_csv('BillboardLyricData.txt', sep='\\t', encoding='utf-8')\n",
    "all_charts = all_charts.dropna() ## Remove missing data\n",
    "\n",
    "class_mapping = {label:idx for idx,label in enumerate(np.unique(all_charts.chart))}\n",
    "y_raw = all_charts.chart.map(class_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2,max_features=5000,stop_words='english')                              \n",
    "vectorizer.fit(all_charts.lyrics) #Creates the dictionary to convert lyrics to counts\n",
    "word_vector = vectorizer.transform(all_charts.lyrics) #Does actual conversion\n",
    "X_raw = word_vector.todense()\n",
    "\n",
    "#Split of test set, and set aside (dont touch until after we have found the best hyperparameters)\n",
    "X_train_and_valid, X_test, y_train_and_valid, y_test = train_test_split(X_raw, y_raw, test_size=0.3)\n",
    "\n",
    "#Split of validation and train set from the remaining\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_and_valid, y_train_and_valid, test_size=0.3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bic_score(y_actual, y_proba, k, n):\n",
    "    LL = log_loss(y_actual,y_p)\n",
    "    LL = -LL #Because sklearn negates loss by convention\n",
    "    return -2*LL + k*np.log(n)\n",
    "\n",
    "def aic_score(y_actual, y_proba, k):\n",
    "    LL = log_loss(y_actual,y_p)\n",
    "    LL = -LL #Because sklearn negates loss by convention\n",
    "    return -2*LL + k*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIC: 37.3791209744\n",
      "AIC: 13.6706175011\n",
      "Test Acc:  0.195054945055\n",
      "BIC: 71.0508922842\n",
      "AIC: 23.6338853376\n",
      "Test Acc:  0.197802197802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bdyet\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIC: 677.396197228\n",
      "AIC: 203.226127763\n",
      "Test Acc:  0.247252747253\n",
      "BIC: 3372.56477746\n",
      "AIC: 1001.71443014\n",
      "Test Acc:  0.299450549451\n",
      "BIC: 6742.50375501\n",
      "AIC: 2000.80306036\n",
      "Test Acc:  0.315934065934\n",
      "BIC: 30857.4013612\n",
      "AIC: 9154.63728173\n",
      "Test Acc:  0.392857142857\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier(loss='log', n_iter=100, penalty='none')\n",
    "\n",
    "for n_features in [5,10,100,500,1000,5000]:\n",
    "    #X is and ordered list of the [normalized] counts of most frequent words. We can slice this 1000 coloumn matrix of the top 1000 wor\n",
    "    # This will give a training data that is now the top 100 words\n",
    "    X_train_temp = X_train[:,0:n_features]\n",
    "    X_valid_temp = X_valid[:,0:n_features]\n",
    "    model.fit(X_train_temp, y_train) \n",
    "    y_p = model.predict_proba(X_train_temp)\n",
    "    print(\"BIC:\", bic_score(y_train,y_p,X_train_temp.shape[1],X_train_temp.shape[0]))\n",
    "    print(\"AIC:\", aic_score(y_train,y_p,X_train_temp.shape[1]))\n",
    "    print(\"Test Acc: \",model.score(X_valid_temp,y_valid))\n",
    "\n",
    "# #Use 500 features\n",
    "# model_2 = SGDClassifier(loss='log', n_iter=100, penalty='none')\n",
    "# X_train_temp = X_train[:,0:500]\n",
    "# X_valid_temp = X_valid[:,0:500]\n",
    "# model_2.fit(X_train_temp, y_train) \n",
    "# y_p = model_2.predict_proba(X_valid_temp)\n",
    "# print(\"model 2 bic:\", bic_score(y_valid,y_p,X_valid_temp.shape[1],X_valid_temp.shape[0]))\n",
    "# print(\"model 2 aic:\", aic_score(y_valid,y_p,X_valid_temp.shape[1]))\n",
    "# print(model_2.score(X_valid_temp,y_valid))\n",
    "\n",
    "# #Use 500 features\n",
    "# model_2 = SGDClassifier(loss='log', n_iter=100, penalty='none')\n",
    "# X_train_temp = X_train[:,0:1000]\n",
    "# X_valid_temp = X_valid[:,0:1000]\n",
    "# model_2.fit(X_train_temp, y_train) \n",
    "# y_p = model_2.predict_proba(X_valid_temp)\n",
    "# print(\"model 2 bic:\", bic_score(y_valid,y_p,X_valid_temp.shape[1],X_valid_temp.shape[0]))\n",
    "# print(\"model 2 aic:\", aic_score(y_valid,y_p,X_valid_temp.shape[1]))\n",
    "# print(model_2.score(X_valid_temp,y_valid))\n",
    "\n",
    "# #Use all 1000 features\n",
    "# model_3 = SGDClassifier(loss='log', n_iter=100, penalty='none')\n",
    "# X_train_temp = X_train\n",
    "# X_valid_temp = X_valid\n",
    "# model_3.fit(X_train_temp, y_train) \n",
    "# y_p = model_3.predict_proba(X_valid_temp)\n",
    "# print(\"model 3 bic:\", bic_score(y_valid,y_p,X_valid_temp.shape[1],X_valid_temp.shape[0]))\n",
    "# print(\"model 3 aic:\", aic_score(y_valid,y_p,X_valid_temp.shape[1]))\n",
    "# print(model_3.score(X_valid_temp,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
